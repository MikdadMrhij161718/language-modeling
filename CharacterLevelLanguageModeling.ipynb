{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T101kiI6WwfB",
        "outputId": "e9457ba0-3eb6-4812-9a97-1bd505783907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIk3F9BfMbpU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/shakespeare_data.txt'\n",
        "\n",
        "with open(filename) as files:\n",
        "  text = files.read()"
      ],
      "metadata": {
        "id": "xvW3tPFJBN_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters\" , len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqN2cr0MDePf",
        "outputId": "fbe47a17-7928-48e6-bb29-9b29856ba671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters 5283879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwNOh3_gDg2t",
        "outputId": "cde90c78-0da7-4536-89b1-c42832996bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \tA LOVER'S COMPLAINT\n",
            "\n",
            "\n",
            "\n",
            "FROM off a hill whose concave womb reworded\n",
            "A plaintful story from a sistering vale,\n",
            "My spirits to attend this double voice accorded,\n",
            "And down I laid to list the sad-tuned tale;\n",
            "Ere long espied a fickle maid full pale,\n",
            "Tearing of papers, breaking rings a-twain,\n",
            "Storming her world with sorrow's wind and rain.\n",
            "\n",
            "Upon her head a platted hive of straw,\n",
            "Which fortified her visage from the sun,\n",
            "Whereon the thought might think sometime it saw\n",
            "The carcass of beauty spent and done:\n",
            "Time had not scythed all that youth begun,\n",
            "Nor youth all quit; but, spite of heaven's fell rage,\n",
            "Some beauty peep'd through lattice of sear'd age.\n",
            "\n",
            "Oft did she heave her napkin to her eyne,\n",
            "Which on it had conceited characters,\n",
            "Laundering the silken figures in the brine\n",
            "That season'd woe had pelleted in tears,\n",
            "And often reading what contents it bears;\n",
            "As often shrieking undistinguish'd woe,\n",
            "In clamours of all size, both high and low.\n",
            "\n",
            "Sometimes her levell'd eyes their carriage ride,\n",
            "As they d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH9p4l7LM9B9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c784ebe0-eaa3-4895-adb0-e7cd49ac3c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !$&'(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz|\n",
            "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '$': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, ']': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '|': 79}\n",
            "{0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '$', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: ',', 10: '-', 11: '.', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '?', 25: 'A', 26: 'B', 27: 'C', 28: 'D', 29: 'E', 30: 'F', 31: 'G', 32: 'H', 33: 'I', 34: 'J', 35: 'K', 36: 'L', 37: 'M', 38: 'N', 39: 'O', 40: 'P', 41: 'Q', 42: 'R', 43: 'S', 44: 'T', 45: 'U', 46: 'V', 47: 'W', 48: 'X', 49: 'Y', 50: 'Z', 51: '[', 52: ']', 53: 'a', 54: 'b', 55: 'c', 56: 'd', 57: 'e', 58: 'f', 59: 'g', 60: 'h', 61: 'i', 62: 'j', 63: 'k', 64: 'l', 65: 'm', 66: 'n', 67: 'o', 68: 'p', 69: 'q', 70: 'r', 71: 's', 72: 't', 73: 'u', 74: 'v', 75: 'w', 76: 'x', 77: 'y', 78: 'z', 79: '|'}\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "print(''.join(vocab))\n",
        "mapchartoint = {ch:i for i,ch in enumerate(vocab)}\n",
        "mapinttochar = {i:ch for i,ch in enumerate(vocab)}\n",
        "print(mapchartoint)\n",
        "print(mapinttochar)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s : [mapchartoint[c] for c in s] # encode\n",
        "decode = lambda l : ''.join([mapinttochar[i] for i in l]) # decode"
      ],
      "metadata": {
        "id": "6uAW27GUEc4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"Hello\"))\n",
        "print(decode(encode(\"Hello\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN7wTGltEhS-",
        "outputId": "985adf37-7f67-48d9-df89-43e9d92cf1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32, 57, 64, 64, 67]\n",
            "Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(mapchartoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhStYS-qHAl4",
        "outputId": "91ffe940-9fab-4d14-be57-e5d23f242b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text),dtype=torch.long)"
      ],
      "metadata": {
        "id": "bsroBwHhFAYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_text = data[:n]\n",
        "eval_text = data[n:]\n",
        "\n",
        "print(f\"Number of training lines: {len(train_text)}\")\n",
        "print(f\"Number of validation lines: {len(eval_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRcD7nPSBwkW",
        "outputId": "2026648d-6a90-4687-ea43-61a956f0dbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training lines: 4755491\n",
            "Number of validation lines: 528388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpPL7g0GFR2Y",
        "outputId": "11018861-b49f-45ed-c638-680a4434f3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([60, 67, 73,  2, 75, 60, 57, 72,  6, 71, 72,  2, 53,  2, 63, 66, 61, 58,\n",
              "        57,  2, 72, 67,  2, 63, 61, 64, 64,  2, 72, 60, 77, 71, 57, 64, 58, 11,\n",
              "         1,  0, 44, 60, 57,  2, 72, 61, 65, 57,  2, 75, 61, 64, 64,  2, 55, 67,\n",
              "        65, 57,  2, 75, 60, 57, 66,  2, 72, 60, 67, 73,  2, 71, 60, 53, 64, 72,\n",
              "         2, 75, 61, 71, 60,  2, 58, 67, 70,  2, 65, 57,  1,  0, 44, 67,  2, 60,\n",
              "        57, 64, 68,  2, 72, 60, 57, 57,  2, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_5qZPqVQ0jK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "\n",
        "      self.data = data\n",
        "      self.block_size = block_size\n",
        "      self.source , self.labels = self.create_data(self.data,self.block_size)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.data)//self.block_size - 1\n",
        "\n",
        "    def create_data(self , data , block_size):\n",
        "      source_lines = []\n",
        "      labels_lines = []\n",
        "      for i in range(0,len(data),block_size):\n",
        "        line = data[i:i+block_size]\n",
        "        label = data[i+1:block_size+i+1]\n",
        "        if len(line) < block_size:\n",
        "          continue\n",
        "        source_lines.append(line)\n",
        "        labels_lines.append(label)\n",
        "      return source_lines , labels_lines\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        return self.source[idx],self.labels[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_text,block_size=8)\n",
        "val_dataset = CustomDataset(eval_text,block_size=8)"
      ],
      "metadata": {
        "id": "NCTMk7N7FD9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_units):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, n_units, batch_first=True)\n",
        "        self.fc = nn.Linear(n_units, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.log_softmax(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "gcADuF3F2xES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "uaPz8NXmQZYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "\n",
        "    directory = os.path.dirname(path)\n",
        "    if not os.path.exists(directory):\n",
        "          os.makedirs(directory)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "    print(f\"Checkpoint saved at epoch {epoch}\")"
      ],
      "metadata": {
        "id": "zTKDHSsrSuAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 80\n",
        "EMBEDDING_DIM = 200\n",
        "N_UNITS = 256\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMModel(VOCAB_SIZE, EMBEDDING_DIM, N_UNITS).to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr= 0.00125)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for inputs, targets in tqdm(train_dataloader):\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        outputs = outputs.view(-1, model.fc.out_features)\n",
        "        targets = targets.view(-1)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss / len(train_dataloader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for inputs, targets in val_dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.view(-1, model.fc.out_features)\n",
        "        targets = targets.view(-1)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs},  Validation Loss: {epoch_loss / len(val_dataloader)}\")\n",
        "    if epoch + 1 == 30:\n",
        "      save_checkpoint(model, optimizer, epoch + 1, f\"/content/drive/MyDrive/checkpoint/CharacterLevelLanguageModeling_t_checkpoint_epoch_{epoch + 1}.pth\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXRR1OTJLNN0",
        "outputId": "64ed64ee-ac93-49fe-c3ab-9dfb85b910fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9289/9289 [00:31<00:00, 295.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Training Loss: 1.826241798492766\n",
            "Epoch 1/2,  Validation Loss: 16.43794580058072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9289/9289 [00:31<00:00, 297.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2, Training Loss: 1.7060079968056467\n",
            "Epoch 2/2,  Validation Loss: 15.355725079774857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/checkpoint/CharacterLevelLanguageModeling_t_checkpoint_epoch_30.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJAjJ8WFc4_r",
        "outputId": "1ad5e7f1-ce68-4d28-ef91-6bda65f28601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-d1658cba843c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"/content/drive/MyDrive/checkpoint/CharacterLevelLanguageModeling_t_checkpoint_epoch_30.pth\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paZrl_ZVc_li",
        "outputId": "728d1e50-c70a-4697-8b51-0385b2e99cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_text(model, start_sequence, char_to_idx, idx_to_char, length=100, temperature=1.0):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    generated_sequence = start_sequence\n",
        "    input_seq = torch.tensor(encode(generated_sequence)).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            output = model(input_seq)\n",
        "            output = output[:, -1, :]  # Get the output of the last time step\n",
        "            output = output / temperature  # Adjust the output by the temperature\n",
        "            probabilities = torch.nn.functional.softmax(output, dim=-1).squeeze()\n",
        "            predicted_idx = torch.multinomial(probabilities, 1).item()\n",
        "            predicted_char = idx_to_char[predicted_idx]\n",
        "\n",
        "            generated_sequence += predicted_char\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[predicted_idx]]).to(device)], dim=1)\n",
        "\n",
        "    return generated_sequence\n",
        "\n",
        "# Example usage:\n",
        "start_sequence = \"i like y\"\n",
        "\n",
        "generated_text = generate_text(model, start_sequence, mapchartoint, mapinttochar, length=1000, temperature=0.8)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "Ng-z859xQuQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0578ffe-721a-4fc9-fbc9-8d72212f27ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like your weakness, and free and our Dromio, whose bed her sight\n",
            "\tIs doth leave to me of man. Master Brook, sir,\n",
            "\tWinter, that it damn'd to your highness, and punish Aumer Slender; the last lady's sharp subject\n",
            "\tDeeds he was a gallant delights disposition, burntly thus that hath not the more in nature,\n",
            "\tSo minded; I'll and we all after herself, that you have not mockering.\n",
            "\n",
            "\t[Exeunt TRINCULO]\n",
            "\n",
            "\tThe gauntrous man more sorry, she is? Forest to my sleeve\n",
            "That man's witch you find the love and men's new-deep and do in death be coming.\n",
            "\n",
            "TIMON\tWhat, John Cupid help, in a most things and the field\n",
            "\tAnd leave a month prince in the trim that mangled the former offence holds your sword can see the love,\n",
            "\tSo sure I am no more than another's heart and straight he was the cause with the fair scarcely too round\n",
            "\tAnd never let him well and peace, that have I seen your hour, even to crowns,\n",
            "\tOf our esteems that I have peevisous pleasant kingdom, come to whom of Sicilia.\n",
            "\n",
            "VIOLA\tBy my thoughts and tomb, the s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FN0FgyLjMAbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}