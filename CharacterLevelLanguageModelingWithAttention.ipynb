{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae1HurCFKN0D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/shakespeare_data.txt'\n",
        "\n",
        "with open(filename) as files:\n",
        "  text = files.read()"
      ],
      "metadata": {
        "id": "UAqcxIa6K086"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "id": "ahWI8MwmK05i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "print(''.join(vocab))\n",
        "chartoidx = {char:idx for idx,char in enumerate(vocab)}\n",
        "idxtochar = {idx:char for idx,char in enumerate(vocab)}\n",
        "print(chartoidx)\n",
        "print(idxtochar)"
      ],
      "metadata": {
        "id": "x14ba-4SK02c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode_text = lambda string : [chartoidx[char] for char in string]\n",
        "decode = lambda integer : ''.join([idxtochar[idx] for idx in integer])"
      ],
      "metadata": {
        "id": "AIucLWfcLYpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode_text(\"Hello\"))\n",
        "print(decode(encode_text(\"Hello\")))"
      ],
      "metadata": {
        "id": "kK-SZwj7LBls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode_text(text),dtype=torch.long)"
      ],
      "metadata": {
        "id": "13LP7RgE2uHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(text))\n",
        "train_data = data[:n]\n",
        "eval_data = data[n:]\n",
        "\n",
        "print(f\"Number of training lines: {len(train_data)}\")\n",
        "print(f\"Number of validation lines: {len(eval_data)}\")"
      ],
      "metadata": {
        "id": "p9AUwsxeLB9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomeDataset(Dataset):\n",
        "\n",
        "  def __init__(self , data , block_size):\n",
        "\n",
        "    self.data = data\n",
        "    self.block_size = block_size\n",
        "    self.source_lines , self.target_lines = self.create_data()\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return (len(self.data)//self.block_size - 1)\n",
        "\n",
        "  def create_data(self):\n",
        "\n",
        "    source_lines = []\n",
        "    target_lines = []\n",
        "    for i in range(0,len(self.data),self.block_size):\n",
        "      source_line = self.data[i:i+self.block_size]\n",
        "      target_line = self.data[i+1:self.block_size+i+1]\n",
        "      if len(source_line) < self.block_size:\n",
        "        continue\n",
        "      source_lines.append(source_line)\n",
        "      target_lines.append(target_line)\n",
        "\n",
        "    return source_lines , target_lines\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "    return self.source_lines[idx] , self.target_lines[idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "hQg6tLUJLCT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = CustomeDataset(train_data,block_size=256)\n",
        "# val_dataset = CustomeDataset(eval_data,block_size=256)\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "glLu1RYs0rO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self , d_embed:int , vocab_size:int , block_size:int):\n",
        "    super().__init__()\n",
        "    self.d_embed = d_embed\n",
        "    self.vocab_size = vocab_size\n",
        "    self.block_size = block_size\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size,d_embed)\n",
        "    self.position_embedding = nn.Embedding(block_size, d_embed)\n",
        "\n",
        "    self.register_buffer(\n",
        "        \"position_ids\",\n",
        "        torch.arange(self.block_size).expand((1,-1)),\n",
        "        persistent=False,\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    return (self.position_embedding(self.position_ids)+self.embedding(x)) * math.sqrt(self.d_embed)"
      ],
      "metadata": {
        "id": "Ug7gFFc-KtMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "  def __init__(self , d_model, n_head , dropout:float):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.n_head = n_head\n",
        "\n",
        "    self.qurey = nn.Linear(d_model , d_model)\n",
        "    self.key = nn.Linear(d_model,d_model)\n",
        "    self.value = nn.Linear(d_model,d_model)\n",
        "\n",
        "    self.d_embed = d_model // n_head\n",
        "\n",
        "    self.out = nn.Linear(d_model , d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query,key,value,mask,dropout:nn.Dropout):\n",
        "\n",
        "    d_embed = query.shape[-1]\n",
        "\n",
        "    #[batch_size,n_head,seq_length,d_embed] @ [batch_size,n_head,d_embed,seq_length]\n",
        "    #[batch_size,n_head,seq_length,seq_length]\n",
        "    attention_scores = (query @ key.transpose(-2,-1))/math.sqrt(d_embed)\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask==0,-1e9)\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    #[batch_size,n_head,seq_length,seq_length] @ [batch_size,n_head,seq_length,d_embed]\n",
        "    #[batch_size,n_head,seq_length,d_embed]\n",
        "    return (attention_scores @ value)\n",
        "\n",
        "\n",
        "  def forward(self,query,key,value,mask):\n",
        "\n",
        "    #query,key,value = [batch_size , seq_length , n_embed]\n",
        "    query = self.qurey(query)\n",
        "    key = self.key(key)\n",
        "    value = self.value(value)\n",
        "\n",
        "    #query,key,value = [batch_size,seq_length,n_head,d_embed] -> [batch_size,n_head,seq_length,d_embed]\n",
        "    query = query.view(query.shape[0],query.shape[1],self.n_head,self.d_embed).transpose(1,2)\n",
        "    key = key.view(key.shape[0],key.shape[1],self.n_head,self.d_embed).transpose(1,2)\n",
        "    value = value.view(value.shape[0],value.shape[1],self.n_head,self.d_embed).transpose(1,2)\n",
        "\n",
        "    #[batch_size,n_head,seq_length,d_embed]\n",
        "    x = MultiHeadAttentionBlock.attention(query,key,value,mask,self.dropout)\n",
        "\n",
        "    #[batch_size,seq_length,n_head,d_embed]\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0],-1,self.n_head*self.d_embed)\n",
        "\n",
        "    return self.out(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N1q8gyqFPSYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,n_embed , dropout:float):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_1 = nn.Linear(n_embed,4*n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(4*n_embed,n_embed)\n",
        "\n",
        "  def forward(self , x):\n",
        "\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
      ],
      "metadata": {
        "id": "oTEK9Ze_YsfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self,eps:float = 10**-6):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1))\n",
        "    self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    std = x.std(dim=-1,keepdim=True)\n",
        "    return self.alpha * (x-mean)/(std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "FsTMsXP_COPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "  def __init__(self,dropout:float):\n",
        "\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "rA7tlN_LCf3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,self_attention_block:MultiHeadAttentionBlock , feed_forward_block:FeedForwardBlock , dropout:float):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "\n",
        "  def forward(self,x ,mask):\n",
        "\n",
        "    x = self.residual_connections[0](x,lambda x:self.self_attention_block(x,x,x,mask))\n",
        "    x = self.residual_connections[1](x,self.feed_forward_block)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5ie8ZSisDkcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model:int , vocab_size:int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.proj = nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.proj(x)"
      ],
      "metadata": {
        "id": "k71TVJA4E-qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self,layers:nn.ModuleList):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "H97fJGR_Du6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModeling(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder:Encoder , projection_layer: ProjectionLayer , source_embed:InputEmbeddings):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.projection_layer = projection_layer\n",
        "    self.source_embed = source_embed\n",
        "\n",
        "  def encode(self,source,mask):\n",
        "\n",
        "    source = self.source_embed(source)\n",
        "    return self.encoder(source,mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    # (batch, seq_len, vocab_size)\n",
        "    return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "yV_Z4PMgKQi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BuildModel(d_model:int , n_heads:int , vocab_size:int , N:int , dropout:float,block_size:int):\n",
        "\n",
        "  source_embed = InputEmbeddings(d_model, vocab_size,block_size)\n",
        "\n",
        "  blocks = []\n",
        "  for _ in range(N):\n",
        "    self_attention_block = MultiHeadAttentionBlock(d_model, n_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, dropout)\n",
        "    encoder_block = EncoderBlock(self_attention_block, feed_forward_block, dropout)\n",
        "    blocks.append(encoder_block)\n",
        "  encoder = Encoder(nn.ModuleList(blocks))\n",
        "\n",
        "  projection_layer = ProjectionLayer(d_model, vocab_size)\n",
        "\n",
        "  languagemodeling = LanguageModeling(encoder,projection_layer,source_embed)\n",
        "\n",
        "  return languagemodeling\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1uKZ7JBYFQO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "2JANy8APK0sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "\n",
        "    directory = os.path.dirname(path)\n",
        "    if not os.path.exists(directory):\n",
        "          os.makedirs(directory)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "    print(f\"Checkpoint saved at epoch {epoch}\")"
      ],
      "metadata": {
        "id": "S4aXdlE0M3IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.tril(torch.ones(4, 4))\n",
        "mask = mask.masked_fill(mask==0,float('-inf'))\n",
        "F.softmax(mask, dim=-1)"
      ],
      "metadata": {
        "id": "QgrGNKYn4xeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_epochs = 30\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iter = 200\n",
        "n_embed = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "vocab_size=80\n",
        "train_dataset = CustomeDataset(train_data,block_size=block_size)\n",
        "val_dataset = CustomeDataset(eval_data,block_size=block_size)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "model = BuildModel(d_model=n_embed , n_heads=n_head , vocab_size=vocab_size , N=n_layer , dropout=dropout,block_size=block_size).to(device)\n",
        "print(count_parameters(model))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "mask = torch.tril(torch.ones(block_size, block_size)).to(device)\n",
        "mask = mask.masked_fill(mask==0,float('-inf'))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  for source,target in tqdm(train_dataloader):\n",
        "    source ,target = source.to(device) , target.to(device)\n",
        "\n",
        "    encode = model.encode(source,mask)\n",
        "    logits = model.project(encode)\n",
        "    prob = torch.softmax(logits,dim=-1)\n",
        "\n",
        "    prob = prob.view(-1, vocab_size)\n",
        "    targets = target.view(-1)\n",
        "    loss = loss_fn(prob, targets)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {train_loss / len(train_dataloader)}\")\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for source,target in tqdm(val_dataloader):\n",
        "      source ,target = source.to(device) , target.to(device)\n",
        "\n",
        "      encode = model.encode(source,mask)\n",
        "      logits = model.project(encode)\n",
        "      prob = torch.softmax(logits,dim=-1)\n",
        "\n",
        "      prob = prob.view(-1, vocab_size)\n",
        "      targets = target.view(-1)\n",
        "      loss = loss_fn(prob, targets)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}/{n_epochs},  Validation Loss: {val_loss / len(val_dataloader)}\")\n",
        "  if epoch + 1 == 20 or epoch +1 == 30:\n",
        "      save_checkpoint(model, optimizer, epoch + 1, f\"/content/drive/MyDrive/checkpoint/CharacterLevelLanguageModeling_with_Attention__t_checkpoint_epoch_{epoch + 1}.pth\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RMrOFhmeYshb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/checkpoint/CharacterLevelLanguageModeling_with_Attention__t_checkpoint_epoch_30.pth\")"
      ],
      "metadata": {
        "id": "iA2BisaDaAiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "HNycFpULaLzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_sequence, char_to_idx, idx_to_char, block_size,length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    generated_sequence = start_sequence\n",
        "    input_seq = torch.tensor(encode_text(generated_sequence)).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            input_seq = input_seq[:,-block_size:]\n",
        "            output = model.encode(input_seq,None)\n",
        "            logits = model.project(output)\n",
        "            logits = logits[:, -1, :]\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=-1).squeeze()\n",
        "            predicted_idx = torch.multinomial(logits, 1).item()\n",
        "            predicted_char = idx_to_char[predicted_idx]\n",
        "\n",
        "            generated_sequence += predicted_char\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[predicted_idx]]).to(device)], dim=-1)\n",
        "\n",
        "    return generated_sequence\n",
        "\n",
        "# Example usage:\n",
        "start_sequence = text[-block_size:]\n",
        "generated_text = generate_text(model, start_sequence, chartoidx, idxtochar, block_size,length=1, temperature=0.8)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "mNRBp_LXYskr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmbDjegZXPQF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}